{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_precision (clf):\n",
    "\n",
    "\n",
    "    #Training model\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # make predictions for test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    #calculating precision\n",
    "    precision=precision_score(y_pred,y_test)\n",
    "    \n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, y_prob)\n",
    "\n",
    "    return (precision,ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    xgb.XGBClassifier(),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Precision\",\"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    name = clf.__class__.__name__\n",
    "    precision,ll=model_precision (clf)\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print(\"Precision: {:.4%}\".format(precision))\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_entry = pd.DataFrame([[name,precision,ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Precision', y='Classifier', data=log, color=\"b\")\n",
    "\n",
    "plt.xlabel('Precision %')\n",
    "plt.title('Classifier Precision')\n",
    "plt.show()\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, X_train, y_train,X_test,y_test,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                            metrics='logloss', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train,eval_metric='logloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    y_pred = alg.predict(X_test)\n",
    "    y_predprob = alg.predict_proba(X_train)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report-TEST\")\n",
    "    print (\"Logloss : %.4g\" % metrics.log_loss(y_pred, y_test))\n",
    "    print (\"Precision : %.4g\" % metrics.precision_score(y_pred, y_test))\n",
    "    print (\"AUC Score(Train): %f\" % metrics.roc_auc_score(y_train.values,y_predprob))\n",
    "    \n",
    "    \n",
    "    feat_imp = pd.DataFrame(list(alg.get_booster().get_fscore().items()),\n",
    "                columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "    sns.barplot(x='feature',y='importance',data=feat_imp)\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5,return_train_score=False)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "print(gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomized search\n",
    "\n",
    "\n",
    "xgb3 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=7,\n",
    " min_child_weight=1,\n",
    " gamma=0.2,\n",
    " subsample=0.5,\n",
    " colsample_bytree=0.6,\n",
    " reg_alpha=0.0005,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "parameter_grid={'max_depth':[3,7,9],\n",
    "                 'min_child_weight':[1,3,5],\n",
    "                'gamma':[i/10.0 for i in range(0,5)],\n",
    "                 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    "                 'colsample_bytree':[i/100.0 for i in range(50,70,5)],\n",
    "                'reg_alpha':[5e-06,1e-05,5e-04,1e-04]\n",
    "                    }\n",
    "\n",
    "cross_validation=StratifiedKFold(n_splits=10)\n",
    "\n",
    "grid_search= RandomizedSearchCV(xgb3,\n",
    "                          param_distributions=parameter_grid,\n",
    "                          cv=cross_validation,\n",
    "                            n_iter=100)\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print('Best Score :',grid_search.best_score_)\n",
    "print('Best Parameters : ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
